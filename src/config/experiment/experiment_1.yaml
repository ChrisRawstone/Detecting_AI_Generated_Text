model:
  pretrained_model: 'distilbert-base-uncased'
  num_labels: 2

training_args:
  output_dir: './results'
  num_train_epochs: 3
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 64
  warmup_steps: 50
  weight_decay: 0.01
  logging_dir: './logs'
  logging_steps: 10
